{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "940cbaab",
   "metadata": {},
   "outputs": [],
   "source": "# Basic NLP Preprocessing Guide with NLTK\n\nThis notebook covers essential text preprocessing techniques using NLTK (Natural Language Toolkit) for NLP tasks.\n\n## Table of Contents\n1. Installation & Setup\n2. Text Cleaning\n3. Tokenization\n4. Lowercasing\n5. Removing Punctuation & Special Characters\n6. Removing Stopwords\n7. Stemming\n8. Lemmatization\n9. Complete Preprocessing Pipeline\n10. Best Practices & When to Use Each Technique"
  },
  {
   "cell_type": "markdown",
   "id": "f9ot5ih858",
   "source": "## 1. Installation & Setup",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "82ci5r0z4b7",
   "source": "# Install NLTK (if not already installed)\n# !pip install nltk\n\nimport nltk\nimport re\nimport string\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\n\n# Download required NLTK data\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\nnltk.download('averaged_perceptron_tagger')\n\nprint(\"NLTK setup complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "13k60iuc7hvm",
   "source": "## 2. Sample Text for Demonstration",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "798vbf31t1y",
   "source": "# Sample text with various challenges\ntext = \"\"\"\nNatural Language Processing (NLP) is AMAZING!!! It's a subfield of AI \nthat focuses on interaction between computers and humans. \nNLP techniques include: tokenization, stemming, lemmatization, etc.\nEmail me at example@email.com or visit https://www.example.com for more info.\nPrices: $100, â‚¬50, Â£30. Phone: +1-234-567-8900.\n\"\"\"\n\nprint(\"Original Text:\")\nprint(text)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "khztdxnw4h",
   "source": "## 3. Text Cleaning\n\nRemove unwanted elements like URLs, emails, special characters, numbers, and extra whitespace.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "d167jv60a3o",
   "source": "def clean_text(text):\n    \"\"\"\n    Clean text by removing URLs, emails, special characters, numbers, and extra whitespace\n    \"\"\"\n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    \n    # Remove email addresses\n    text = re.sub(r'\\S+@\\S+', '', text)\n    \n    # Remove phone numbers\n    text = re.sub(r'\\+?\\d[\\d -]{8,}\\d', '', text)\n    \n    # Remove currency symbols and amounts\n    text = re.sub(r'[$â‚¬Â£]\\d+', '', text)\n    \n    # Remove extra whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text\n\ncleaned_text = clean_text(text)\nprint(\"Cleaned Text:\")\nprint(cleaned_text)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "vit0xud3x5k",
   "source": "## 4. Tokenization\n\nBreak text into words (word tokenization) or sentences (sentence tokenization).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "4cknrhvwc03",
   "source": "# Sentence tokenization\nsentences = sent_tokenize(cleaned_text)\nprint(\"Sentence Tokenization:\")\nfor i, sent in enumerate(sentences, 1):\n    print(f\"{i}. {sent}\")\n\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# Word tokenization\nwords = word_tokenize(cleaned_text)\nprint(\"Word Tokenization:\")\nprint(words)\nprint(f\"\\nTotal words: {len(words)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cs7edve9nc9",
   "source": "## 5. Lowercasing\n\nConvert all text to lowercase for consistency.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "yiif1uf2lja",
   "source": "lowercase_text = cleaned_text.lower()\nprint(\"Lowercase Text:\")\nprint(lowercase_text)\n\n# Lowercase tokens\nlowercase_words = [word.lower() for word in words]\nprint(\"\\nLowercase Tokens:\")\nprint(lowercase_words[:10])  # Show first 10",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5yh60xs481o",
   "source": "## 6. Removing Punctuation & Special Characters\n\nRemove punctuation marks and keep only alphabetic characters.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cfydgcqb0wo",
   "source": "# Method 1: Using string.punctuation\ndef remove_punctuation(text):\n    return text.translate(str.maketrans('', '', string.punctuation))\n\nno_punct_text = remove_punctuation(lowercase_text)\nprint(\"Method 1 - Remove Punctuation:\")\nprint(no_punct_text)\n\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# Method 2: Keep only alphabetic words from tokens\nalpha_words = [word for word in lowercase_words if word.isalpha()]\nprint(\"Method 2 - Only Alphabetic Words:\")\nprint(alpha_words)\nprint(f\"\\nWords after removing punctuation: {len(alpha_words)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "kncuf5rgk6r",
   "source": "## 7. Removing Stopwords\n\nRemove common words (like \"the\", \"is\", \"at\") that don't carry much meaning.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "crr4mawlloe",
   "source": "# Get English stopwords\nstop_words = set(stopwords.words('english'))\n\nprint(f\"Total stopwords in NLTK: {len(stop_words)}\")\nprint(f\"Sample stopwords: {list(stop_words)[:20]}\")\n\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# Remove stopwords\nfiltered_words = [word for word in alpha_words if word not in stop_words]\n\nprint(\"Before removing stopwords:\")\nprint(alpha_words)\nprint(f\"\\nWords count: {len(alpha_words)}\")\n\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\nprint(\"After removing stopwords:\")\nprint(filtered_words)\nprint(f\"\\nWords count: {len(filtered_words)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "x67sfj39ov",
   "source": "## 8. Stemming\n\nReduce words to their root/base form by removing suffixes (e.g., \"running\" â†’ \"run\").",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "tp68rcee3uk",
   "source": "# Porter Stemmer (most common)\nporter = PorterStemmer()\n\n# Snowball Stemmer (improved version)\nsnowball = SnowballStemmer('english')\n\n# Test words\ntest_words = ['running', 'runs', 'ran', 'runner', 'easily', 'fairly', \n              'playing', 'played', 'plays', 'connection', 'connections', 'connected']\n\nprint(\"Stemming Comparison:\")\nprint(f\"{'Original':<15} {'Porter':<15} {'Snowball':<15}\")\nprint(\"-\" * 50)\nfor word in test_words:\n    print(f\"{word:<15} {porter.stem(word):<15} {snowball.stem(word):<15}\")\n\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# Apply stemming to our filtered words\nstemmed_words = [porter.stem(word) for word in filtered_words]\nprint(\"Stemmed words:\")\nprint(stemmed_words)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2gmact47xtt",
   "source": "## 9. Lemmatization\n\nConvert words to their dictionary base form (lemma) using linguistic rules (e.g., \"better\" â†’ \"good\").",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "xg7puws5ug",
   "source": "# Initialize lemmatizer\nlemmatizer = WordNetLemmatizer()\n\n# Test words - Lemmatization is context-aware\ntest_words_lem = ['running', 'runs', 'ran', 'better', 'best', 'worse', 'worst',\n                  'caring', 'cares', 'geese', 'feet', 'mice', 'studies', 'studying']\n\nprint(\"Stemming vs Lemmatization Comparison:\")\nprint(f\"{'Original':<15} {'Stemmed':<15} {'Lemmatized':<15}\")\nprint(\"-\" * 50)\nfor word in test_words_lem:\n    print(f\"{word:<15} {porter.stem(word):<15} {lemmatizer.lemmatize(word):<15}\")\n\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# Lemmatization with POS tags (more accurate)\nprint(\"Lemmatization with POS tags (Part-of-Speech):\")\nprint(f\"{'Word':<15} {'Without POS':<15} {'With POS (verb)':<20}\")\nprint(\"-\" * 55)\n\nfor word in ['running', 'better', 'caring', 'studying']:\n    no_pos = lemmatizer.lemmatize(word)\n    with_pos = lemmatizer.lemmatize(word, pos='v')  # 'v' = verb\n    print(f\"{word:<15} {no_pos:<15} {with_pos:<20}\")\n\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# Apply lemmatization to our filtered words\nlemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\nprint(\"Lemmatized words:\")\nprint(lemmatized_words)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ey9dewsis5f",
   "source": "## 10. Complete Preprocessing Pipeline\n\nCombine all steps into a reusable preprocessing function.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "vp6e3tbc8ip",
   "source": "def preprocess_text(text, \n                    remove_urls=True,\n                    remove_emails=True, \n                    lowercase=True,\n                    remove_punctuation=True,\n                    remove_stopwords=True,\n                    use_stemming=False,\n                    use_lemmatization=True):\n    \"\"\"\n    Complete text preprocessing pipeline\n    \n    Parameters:\n    -----------\n    text : str\n        Input text to preprocess\n    remove_urls : bool\n        Remove URLs from text\n    remove_emails : bool\n        Remove email addresses\n    lowercase : bool\n        Convert to lowercase\n    remove_punctuation : bool\n        Remove punctuation marks\n    remove_stopwords : bool\n        Remove common stopwords\n    use_stemming : bool\n        Apply stemming (choose stemming OR lemmatization, not both)\n    use_lemmatization : bool\n        Apply lemmatization\n    \n    Returns:\n    --------\n    str or list : Preprocessed text\n    \"\"\"\n    \n    # 1. Clean text\n    if remove_urls:\n        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    \n    if remove_emails:\n        text = re.sub(r'\\S+@\\S+', '', text)\n    \n    # Remove extra whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    # 2. Lowercase\n    if lowercase:\n        text = text.lower()\n    \n    # 3. Tokenize\n    tokens = word_tokenize(text)\n    \n    # 4. Remove punctuation (keep only alphabetic)\n    if remove_punctuation:\n        tokens = [word for word in tokens if word.isalpha()]\n    \n    # 5. Remove stopwords\n    if remove_stopwords:\n        stop_words = set(stopwords.words('english'))\n        tokens = [word for word in tokens if word not in stop_words]\n    \n    # 6. Stemming or Lemmatization\n    if use_stemming:\n        stemmer = PorterStemmer()\n        tokens = [stemmer.stem(word) for word in tokens]\n    elif use_lemmatization:\n        lemmatizer = WordNetLemmatizer()\n        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n    \n    return tokens\n\n\n# Test the pipeline\nsample_text = \"\"\"\nMachine Learning is AWESOME!!! It's revolutionizing industries.\nCheck out https://example.com for more info. Contact: test@email.com\n\"\"\"\n\nprint(\"Original Text:\")\nprint(sample_text)\nprint(\"\\n\" + \"=\"*70 + \"\\n\")\n\n# Process with different configurations\nprint(\"1. Full Preprocessing (with lemmatization):\")\nresult1 = preprocess_text(sample_text)\nprint(result1)\n\nprint(\"\\n\" + \"=\"*70 + \"\\n\")\n\nprint(\"2. With Stemming instead of Lemmatization:\")\nresult2 = preprocess_text(sample_text, use_stemming=True, use_lemmatization=False)\nprint(result2)\n\nprint(\"\\n\" + \"=\"*70 + \"\\n\")\n\nprint(\"3. Minimal Preprocessing (just lowercase and tokenize):\")\nresult3 = preprocess_text(sample_text, \n                          remove_stopwords=False, \n                          use_lemmatization=False)\nprint(result3)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7b5dotoyoqm",
   "source": "## 11. Best Practices & When to Use Each Technique\n\n### Summary Table\n\n| Technique | Purpose | When to Use | When NOT to Use |\n|-----------|---------|-------------|-----------------|\n| **Lowercasing** | Normalize text | Almost always | Named Entity Recognition, sentiment (ALL CAPS matters) |\n| **Remove Punctuation** | Reduce noise | Topic modeling, classification | Sentiment analysis (! matters), code analysis |\n| **Remove Stopwords** | Focus on content words | Topic modeling, search engines | Sentiment analysis, machine translation |\n| **Stemming** | Fast normalization | Search engines, large datasets | When precision matters (medical, legal) |\n| **Lemmatization** | Accurate normalization | Classification, NER, Q&A systems | When speed is critical |\n| **Tokenization** | Break into units | Always (first step) | - |\n\n### Stemming vs Lemmatization\n\n| Aspect | Stemming | Lemmatization |\n|--------|----------|---------------|\n| **Speed** | Fast | Slower |\n| **Accuracy** | Less accurate | More accurate |\n| **Output** | May not be real words | Always real words |\n| **Dictionary** | Rule-based | Uses dictionary |\n| **Example** | \"studies\" â†’ \"studi\" | \"studies\" â†’ \"study\" |\n| **Use case** | Search engines, IR | NLP tasks, classification |\n\n### Common Preprocessing Pipelines\n\n#### 1. Text Classification (e.g., Spam Detection)\n```python\npreprocess_text(text, \n                remove_stopwords=True,\n                use_lemmatization=True)\n```\n\n#### 2. Sentiment Analysis\n```python\npreprocess_text(text,\n                lowercase=True,\n                remove_stopwords=False,  # \"not good\" needs \"not\"\n                remove_punctuation=False,  # !!! matters\n                use_lemmatization=True)\n```\n\n#### 3. Topic Modeling\n```python\npreprocess_text(text,\n                remove_stopwords=True,\n                use_stemming=True,  # Faster, good enough\n                remove_punctuation=True)\n```\n\n#### 4. Named Entity Recognition (NER)\n```python\npreprocess_text(text,\n                lowercase=False,  # \"Apple\" vs \"apple\"\n                remove_stopwords=False,\n                remove_punctuation=False,\n                use_lemmatization=False)\n```\n\n### Key Takeaways\n\n1. **There's no one-size-fits-all** - Adjust preprocessing based on your task\n2. **Start simple** - Begin with basic preprocessing, add complexity if needed\n3. **Don't over-preprocess** - Removing too much can hurt performance\n4. **Test impact** - Measure how each preprocessing step affects your model\n5. **Domain matters** - Medical/legal text needs different preprocessing than tweets\n6. **Keep original data** - Always preserve raw text for reference",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "jkyq3tceqh",
   "source": "## 12. Bonus: Advanced Techniques\n\n### N-grams (Bigrams, Trigrams)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ok45d1nzohc",
   "source": "from nltk import bigrams, trigrams\nfrom collections import Counter\n\ntext_sample = \"Natural language processing is a subfield of artificial intelligence\"\ntokens = word_tokenize(text_sample.lower())\n\n# Unigrams (single words)\nunigrams = tokens\nprint(\"Unigrams:\", unigrams)\n\n# Bigrams (pairs of consecutive words)\nbigram_list = list(bigrams(tokens))\nprint(\"\\nBigrams:\", bigram_list)\n\n# Trigrams (triplets of consecutive words)\ntrigram_list = list(trigrams(tokens))\nprint(\"\\nTrigrams:\", trigram_list)\n\n# Most common bigrams\ntext_long = \"machine learning is great. machine learning is powerful. deep learning is amazing\"\ntokens_long = word_tokenize(text_long.lower())\nbigrams_long = list(bigrams(tokens_long))\n\nbigram_freq = Counter(bigrams_long)\nprint(\"\\nMost common bigrams:\")\nfor bigram, count in bigram_freq.most_common(5):\n    print(f\"{bigram}: {count}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1v9odmbdddv",
   "source": "### Part-of-Speech (POS) Tagging",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "h61mtiqt40b",
   "source": "from nltk import pos_tag\n\n# POS tagging helps identify grammatical roles\ntext_pos = \"The quick brown fox jumps over the lazy dog\"\ntokens_pos = word_tokenize(text_pos)\npos_tags = pos_tag(tokens_pos)\n\nprint(\"POS Tagging:\")\nprint(f\"{'Word':<15} {'POS Tag':<10} {'Meaning':<20}\")\nprint(\"-\" * 50)\n\n# Common POS tags\npos_meanings = {\n    'DT': 'Determiner',\n    'JJ': 'Adjective',\n    'NN': 'Noun (singular)',\n    'NNS': 'Noun (plural)',\n    'VB': 'Verb (base form)',\n    'VBZ': 'Verb (3rd person)',\n    'IN': 'Preposition',\n    'RB': 'Adverb',\n    'PRP': 'Pronoun'\n}\n\nfor word, tag in pos_tags:\n    meaning = pos_meanings.get(tag, 'Other')\n    print(f\"{word:<15} {tag:<10} {meaning:<20}\")\n\n# Extract only nouns\nnouns = [word for word, pos in pos_tags if pos.startswith('NN')]\nprint(f\"\\nNouns found: {nouns}\")\n\n# Extract only verbs\nverbs = [word for word, pos in pos_tags if pos.startswith('VB')]\nprint(f\"Verbs found: {verbs}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "dpduvg2o",
   "source": "### Handling Contractions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "aj6nxygs048",
   "source": "# Expanding contractions (manual approach)\ncontractions_dict = {\n    \"don't\": \"do not\",\n    \"can't\": \"cannot\",\n    \"won't\": \"will not\",\n    \"shouldn't\": \"should not\",\n    \"i'm\": \"i am\",\n    \"you're\": \"you are\",\n    \"it's\": \"it is\",\n    \"that's\": \"that is\",\n    \"haven't\": \"have not\",\n    \"hasn't\": \"has not\",\n    \"hadn't\": \"had not\",\n    \"wouldn't\": \"would not\",\n    \"weren't\": \"were not\",\n    \"isn't\": \"is not\",\n}\n\ndef expand_contractions(text, contractions_dict):\n    \"\"\"Expand contractions in text\"\"\"\n    for contraction, expansion in contractions_dict.items():\n        text = text.replace(contraction, expansion)\n        text = text.replace(contraction.capitalize(), expansion.capitalize())\n    return text\n\ntext_with_contractions = \"I don't think it's gonna work. We can't do this, shouldn't we?\"\nprint(\"Original:\", text_with_contractions)\nprint(\"Expanded:\", expand_contractions(text_with_contractions.lower(), contractions_dict))\n\n# Note: For production, use the 'contractions' library:\n# pip install contractions\n# import contractions\n# contractions.fix(\"I don't think it's gonna work\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d5prfv0gcnh",
   "source": "## 13. Real-World Example: Processing Multiple Documents",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ot37rcsvjrm",
   "source": "# Sample documents (like tweets, reviews, articles)\ndocuments = [\n    \"I LOVE this product!!! Best purchase ever!!!\",\n    \"Terrible experience. Would NOT recommend. Very disappointed.\",\n    \"It's okay, nothing special. Average quality for the price.\",\n    \"Amazing! Fast delivery and great customer service ðŸ˜Š\",\n    \"Don't buy this. Complete waste of money. Regret it.\"\n]\n\nprint(\"Processing Multiple Documents\")\nprint(\"=\" * 70)\n\n# Process all documents\nprocessed_docs = []\nfor i, doc in enumerate(documents, 1):\n    print(f\"\\n{i}. Original: {doc}\")\n    \n    # Preprocess\n    processed = preprocess_text(doc, \n                                remove_stopwords=False,  # Keep stopwords for sentiment\n                                use_lemmatization=True)\n    \n    processed_docs.append(processed)\n    print(f\"   Processed: {' '.join(processed)}\")\n\nprint(\"\\n\" + \"=\" * 70)\n\n# Create vocabulary (unique words across all documents)\nvocabulary = set()\nfor doc in processed_docs:\n    vocabulary.update(doc)\n\nprint(f\"\\nTotal vocabulary size: {len(vocabulary)}\")\nprint(f\"Vocabulary sample: {list(vocabulary)[:20]}\")\n\n# Word frequency across all documents\nfrom collections import Counter\nall_words = [word for doc in processed_docs for word in doc]\nword_freq = Counter(all_words)\n\nprint(f\"\\nTop 10 most common words:\")\nfor word, count in word_freq.most_common(10):\n    print(f\"  {word}: {count}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "figfs68vywg",
   "source": "## 14. Common Pitfalls and Tips\n\n### âš ï¸ Common Mistakes\n\n1. **Over-preprocessing**: Removing too much information\n   - Example: Removing all punctuation for sentiment analysis loses \"!!!\" emphasis\n   \n2. **Wrong order**: Stemming before removing stopwords\n   - Always remove stopwords BEFORE stemming/lemmatization\n   \n3. **Not handling case consistently**: Mixing \"The\" and \"the\"\n   - Always lowercase early in the pipeline\n   \n4. **Ignoring domain-specific terms**: Medical abbreviations, technical jargon\n   - Create custom stopword lists for your domain\n   \n5. **Not preserving original text**: Always keep the raw version\n   - You might need it for debugging or re-processing\n\n### âœ… Best Practices\n\n1. **Start with minimal preprocessing** and add steps as needed\n2. **Test each preprocessing step's impact** on model performance\n3. **Use lemmatization over stemming** unless speed is critical\n4. **Keep a preprocessing log** to track what was done\n5. **Validate preprocessing output** manually on sample data\n6. **Consider the task**: Different NLP tasks need different preprocessing\n\n### ðŸ“š Additional Resources\n\n- NLTK Documentation: https://www.nltk.org/\n- SpaCy (alternative to NLTK): https://spacy.io/\n- TextBlob (simpler API): https://textblob.readthedocs.io/\n- Hugging Face Tokenizers: https://huggingface.co/docs/tokenizers/\n\n### ðŸŽ¯ Next Steps\n\n1. Try preprocessing on your own text data\n2. Experiment with different combinations of techniques\n3. Measure preprocessing impact on your model\n4. Explore spaCy for production-grade preprocessing\n5. Learn about advanced techniques: NER, dependency parsing, coreference resolution",
   "metadata": {}
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}