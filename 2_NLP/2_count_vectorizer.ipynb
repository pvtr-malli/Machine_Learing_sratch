{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6778d85c",
   "metadata": {},
   "source": [
    "\n",
    "## 1️⃣ CountVectorizer\n",
    "\n",
    "### **Formula**\n",
    "\n",
    "For a document ( d ) and word ( w ):\n",
    "\n",
    "$$\n",
    "[\n",
    "\\text{Count}(w, d) = \\text{number of times } w \\text{ appears in } d\n",
    "]\n",
    "$$\n",
    "---\n",
    "\n",
    "### **Small Example**\n",
    "\n",
    "**Documents**\n",
    "\n",
    "```\n",
    "D1: \"machine learning is fun machine\"\n",
    "D2: \"machine learning is powerful\"\n",
    "```\n",
    "\n",
    "**Vocabulary (V)**\n",
    "\n",
    "```\n",
    "[\"machine\", \"learning\", \"is\", \"fun\", \"powerful\"]\n",
    "```\n",
    "\n",
    "**Count Vectors**\n",
    "\n",
    "| Word     | D1 | D2 |\n",
    "| -------- | -- | -- |\n",
    "| machine  | 2  | 1  |\n",
    "| learning | 1  | 1  |\n",
    "| is       | 1  | 1  |\n",
    "| fun      | 1  | 0  |\n",
    "| powerful | 0  | 1  |\n",
    "\n",
    "So:\n",
    "\n",
    "```\n",
    "D1 → [2, 1, 1, 1, 0]\n",
    "D2 → [1, 1, 1, 0, 1]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Bag of Words → the idea\n",
    "\n",
    "CountVectorizer → BoW implementation using counts\n",
    "\n",
    "TF-IDF → BoW + weighting\n",
    "\n",
    "Binary BoW → 0/1 (present or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d866d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "class CountVectorizer:\n",
    "    \"\"\"\n",
    "    A simple Count Vectorizer implemented from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.vocabulary_: dict[str, int] = {} # word -> index\n",
    "        self.fitted: bool = False\n",
    "\n",
    "    def _tokenize(self, text: str) -> list[str]:\n",
    "        return text.lower().split()\n",
    "\n",
    "    def fit(self, documents: list[str]) -> None:\n",
    "        \"\"\"\n",
    "        Learn the vocabulary from the documents.\n",
    "        \"\"\"\n",
    "        tokenized_docs = [self._tokenize(doc) for doc in documents]\n",
    "\n",
    "        vocab = set()\n",
    "        for tokens in tokenized_docs:\n",
    "            vocab.update(tokens)\n",
    "\n",
    "        # vocabulary should have this word:index format.\n",
    "        self.vocabulary_ = {\n",
    "            word: idx for idx, word in enumerate(sorted(vocab))\n",
    "        }\n",
    "\n",
    "        self.fitted = True\n",
    "\n",
    "    def transform(self, documents: list[str]) -> list[list[int]]:\n",
    "        \"\"\"\n",
    "        Transform documents to count vectors.\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"The vectorizer must be fitted before calling transform().\")\n",
    "\n",
    "        vectors: list[list[int]] = []\n",
    "\n",
    "        for doc in documents:\n",
    "            tokens = self._tokenize(doc)\n",
    "            token_counts = Counter(tokens)\n",
    "\n",
    "            vector = [0] * len(self.vocabulary_)\n",
    "\n",
    "            for word, count in token_counts.items():\n",
    "                if word in self.vocabulary_:\n",
    "                    idx = self.vocabulary_[word]\n",
    "                    vector[idx] = count\n",
    "\n",
    "            vectors.append(vector)\n",
    "\n",
    "        return vectors\n",
    "\n",
    "    def fit_transform(self, documents: list[str]) -> list[list[int]]:\n",
    "        \"\"\"\n",
    "        Fit to data, then transform it.\n",
    "        \"\"\"\n",
    "        self.fit(documents)\n",
    "        return self.transform(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec9180dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 1, 2, 1]\n",
      "[1, 0, 1, 0, 1, 1, 0]\n",
      "[0, 1, 0, 1, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "documents = [\n",
    "    \"I love machine learning love\",\n",
    "    \"I love deep learning\",\n",
    "    \"Machine learning is fun\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "for vec in tfidf_matrix:\n",
    "    print(vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d26d71",
   "metadata": {},
   "source": [
    "| Function        | Time Complexity | Space Complexity | Explanation                            |\n",
    "| --------------- | --------------- | ---------------- | -------------------------------------- |\n",
    "| `_tokenize`     | O(L)            | O(L)             | Splits text into tokens                |\n",
    "| `fit`           | O(N·L)          | O(V)             | Scans all tokens to build vocabulary   |\n",
    "| `transform`     | O(N·(L + V))    | O(N·V)           | Counts tokens and builds dense vectors |\n",
    "| `fit_transform` | O(N·(L + V))    | O(N·V)           | `fit` + `transform` combined           |\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff710db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
