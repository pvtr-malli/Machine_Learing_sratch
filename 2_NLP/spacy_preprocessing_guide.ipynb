{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production-Grade NLP Preprocessing with SpaCy\n",
    "\n",
    "**SpaCy is the industry standard for production NLP** - 10-100x faster than NLTK.\n",
    "\n",
    "## Why SpaCy Over NLTK?\n",
    "\n",
    "| Feature | SpaCy ‚úÖ | NLTK ‚ùå |\n",
    "|---------|---------|--------|\n",
    "| **Speed** | 10-100x faster | Slow |\n",
    "| **Design** | Production-ready | Research/Education |\n",
    "| **API** | Clean, consistent | Fragmented |\n",
    "| **Pre-trained Models** | State-of-the-art | Minimal |\n",
    "| **NER** | Excellent, built-in | Basic |\n",
    "| **Dependency Parsing** | Built-in | Limited |\n",
    "| **Companies Using** | Google, Meta, Microsoft | Academic mainly |\n",
    "| **Memory** | Optimized | Higher usage |\n",
    "\n",
    "## Table of Contents\n",
    "1. Installation & Setup\n",
    "2. Basic Pipeline\n",
    "3. Tokenization\n",
    "4. Lemmatization (No Stemming in SpaCy)\n",
    "5. POS Tagging\n",
    "6. Named Entity Recognition (NER)\n",
    "7. Stopwords & Punctuation\n",
    "8. Dependency Parsing\n",
    "9. Custom Preprocessing Pipeline\n",
    "10. Production Best Practices\n",
    "11. Performance Optimization\n",
    "12. Comparison: NLTK vs SpaCy Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://artifactory.f-sos.net/artifactory/api/pypi/pypi/simple\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "‚úì SpaCy version: 3.8.11\n",
      "‚úì Model loaded: en_core_web_sm\n",
      "\n",
      "Pipeline components: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "\n",
      "Default pipeline stages:\n",
      "  tok2vec         ‚Üí Tok2Vec\n",
      "  tagger          ‚Üí Tagger\n",
      "  parser          ‚Üí DependencyParser\n",
      "  attribute_ruler ‚Üí AttributeRuler\n",
      "  lemmatizer      ‚Üí EnglishLemmatizer\n",
      "  ner             ‚Üí EntityRecognizer\n"
     ]
    }
   ],
   "source": [
    "# Install SpaCy\n",
    "# !pip install spacy\n",
    "\n",
    "# Download models (choose based on your needs)\n",
    "# !python -m spacy download en_core_web_sm   # 12MB - Fast, good for most tasks ‚≠ê\n",
    "# !python -m spacy download en_core_web_md   # 40MB - Includes word vectors\n",
    "# !python -m spacy download en_core_web_lg   # 560MB - Better accuracy\n",
    "# !python -m spacy download en_core_web_trf  # 400MB - Best accuracy (transformer)\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import time\n",
    "\n",
    "# Load model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(f\"‚úì SpaCy version: {spacy.__version__}\")\n",
    "print(f\"‚úì Model loaded: en_core_web_sm\")\n",
    "print(f\"\\nPipeline components: {nlp.pipe_names}\")\n",
    "print(\"\\nDefault pipeline stages:\")\n",
    "for name, component in nlp.pipeline:\n",
    "    print(f\"  {name:15s} ‚Üí {type(component).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Usage - One Line Does Everything!\n",
    "\n",
    "Unlike NLTK where you chain multiple functions, SpaCy does **everything in one call**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Text processed!\n",
      "\n",
      "\n",
      "Apple Inc. was founded by Steve Jobs in Cupertino, California on April 1, 1976. \n",
      "The company is now worth over $2.5 trillion! It's revolutionizing technology.\n",
      "Email: contact@apple.com | Website: https://www.apple.com\n",
      "\n",
      "Total tokens: 43\n",
      "Total sentences: 5\n",
      "Total entities: 6\n",
      "\n",
      "Type of result: <class 'spacy.tokens.doc.Doc'>\n",
      "Doc objects contain ALL linguistic information!\n"
     ]
    }
   ],
   "source": [
    "# Sample text\n",
    "text = \"\"\"\n",
    "Apple Inc. was founded by Steve Jobs in Cupertino, California on April 1, 1976. \n",
    "The company is now worth over $2.5 trillion! It's revolutionizing technology.\n",
    "Email: contact@apple.com | Website: https://www.apple.com\n",
    "\"\"\"\n",
    "\n",
    "# ONE call processes everything!\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"‚úì Text processed!\\n\")\n",
    "print(doc)\n",
    "print(f\"Total tokens: {len(doc)}\")\n",
    "print(f\"Total sentences: {len(list(doc.sents))}\")\n",
    "print(f\"Total entities: {len(doc.ents)}\")\n",
    "print(f\"\\nType of result: {type(doc)}\")\n",
    "print(\"Doc objects contain ALL linguistic information!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Apple Inc. was founded by Steve Jobs in Cupertino, California on April 1, 1976. \n",
      "\n",
      "The company is now worth over $2.5 trillion!\n",
      "It's revolutionizing technology.\n",
      "\n",
      "Email: contact@apple.com\n",
      "| Website: https://www.apple.com\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in doc.sents:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Inc.\n",
      "Steve Jobs\n",
      "Cupertino\n",
      "California\n",
      "April 1, 1976\n",
      "over $2.5 trillion\n"
     ]
    }
   ],
   "source": [
    "for i in doc.ents:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenization - Context-Aware & Smart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy Tokenization (Context-Aware):\n",
      "\n",
      "Text:   Dr. Smith isn't here. He's at N.Y.U.\n",
      "Tokens: ['Dr.', 'Smith', 'is', \"n't\", 'here', '.', 'He', \"'s\", 'at', 'N.Y.U.']\n",
      "\n",
      "Text:   Email: test@email.com\n",
      "Tokens: ['Email', ':', 'test@email.com']\n",
      "\n",
      "Text:   Price: $100.50\n",
      "Tokens: ['Price', ':', '$', '100.50']\n",
      "\n",
      "Text:   Website: https://example.com\n",
      "Tokens: ['Website', ':', 'https://example.com']\n",
      "\n",
      "Text:   don't won't can't\n",
      "Tokens: ['do', \"n't\", 'wo', \"n't\", 'ca', \"n't\"]\n",
      "\n",
      "\n",
      "Token Attributes:\n",
      "Token      Is Alpha   Is Digit   Is Stop    Is Punct  \n",
      "-------------------------------------------------------\n",
      "The        True       False      True       False     \n",
      "quick      True       False      False      False     \n",
      "brown      True       False      False      False     \n",
      "fox        True       False      False      False     \n",
      "jumps      True       False      False      False     \n",
      "over       True       False      True       False     \n",
      "123        False      True       False      False     \n",
      "dogs       True       False      False      False     \n",
      "!          False      False      False      True      \n"
     ]
    }
   ],
   "source": [
    "# SpaCy handles edge cases intelligently\n",
    "test_texts = [\n",
    "    \"Dr. Smith isn't here. He's at N.Y.U.\",\n",
    "    \"Email: test@email.com\",\n",
    "    \"Price: $100.50\",\n",
    "    \"Website: https://example.com\",\n",
    "    \"don't won't can't\"\n",
    "]\n",
    "\n",
    "print(\"SpaCy Tokenization (Context-Aware):\\n\")\n",
    "for text in test_texts:\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    print(f\"Text:   {text}\")\n",
    "    print(f\"Tokens: {tokens}\\n\")\n",
    "\n",
    "# Token attributes\n",
    "doc = nlp(\"The quick brown fox jumps over 123 dogs!\")\n",
    "print(\"\\nToken Attributes:\")\n",
    "print(f\"{'Token':<10} {'Is Alpha':<10} {'Is Digit':<10} {'Is Stop':<10} {'Is Punct':<10}\")\n",
    "print(\"-\" * 55)\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<10} {str(token.is_alpha):<10} {str(token.is_digit):<10} {str(token.is_stop):<10} {str(token.is_punct):<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lemmatization - No Stemming in SpaCy!\n",
    "\n",
    "SpaCy uses **ONLY lemmatization** (more accurate than stemming). Lemmas are always real words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization Results:\n",
      "Original        Lemma           POS       \n",
      "---------------------------------------------\n",
      "running         run             VERB      \n",
      "runs            run             NOUN      \n",
      "ran             run             VERB      \n",
      "runner          runner          NOUN      \n",
      "better          well            ADV       \n",
      "best            good            ADJ       \n",
      "good            good            ADJ       \n",
      "studies         study           NOUN      \n",
      "studying        study           VERB      \n",
      "studied         study           VERB      \n",
      "mice            mouse           NOUN      \n",
      "geese           geese           ADJ       \n",
      "feet            foot            NOUN      \n",
      "children        child           NOUN      \n",
      "\n",
      "Original:   The runners were running faster than they ran yesterday\n",
      "Lemmatized: the runner be run fast than they run yesterday\n"
     ]
    }
   ],
   "source": [
    "# Test words\n",
    "test_words = [\n",
    "    \"running\", \"runs\", \"ran\", \"runner\",\n",
    "    \"better\", \"best\", \"good\",\n",
    "    \"studies\", \"studying\", \"studied\",\n",
    "    \"mice\", \"geese\", \"feet\", \"children\"\n",
    "]\n",
    "\n",
    "doc = nlp(\" \".join(test_words))\n",
    "\n",
    "print(\"Lemmatization Results:\")\n",
    "print(f\"{'Original':<15} {'Lemma':<15} {'POS':<10}\")\n",
    "print(\"-\" * 45)\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<15} {token.lemma_:<15} {token.pos_:<10}\")\n",
    "\n",
    "# Apply to sentence\n",
    "sentence = \"The runners were running faster than they ran yesterday\"\n",
    "doc = nlp(sentence)\n",
    "lemmatized = [token.lemma_ for token in doc]\n",
    "\n",
    "print(f\"\\nOriginal:   {sentence}\")\n",
    "print(f\"Lemmatized: {' '.join(lemmatized)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Part-of-Speech (POS) Tagging - Highly Accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "doc = nlp(sentence)\n",
    "\n",
    "print(\"POS Tagging:\")\n",
    "print(f\"{'Token':<12} {'POS':<8} {'Tag':<8} {'Description':<30}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<12} {token.pos_:<8} {token.tag_:<8} {spacy.explain(token.tag_):<30}\")\n",
    "\n",
    "# Extract by POS\n",
    "nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "verbs = [token.text for token in doc if token.pos_ == \"VERB\"]\n",
    "adjectives = [token.text for token in doc if token.pos_ == \"ADJ\"]\n",
    "\n",
    "print(f\"\\nNouns:      {nouns}\")\n",
    "print(f\"Verbs:      {verbs}\")\n",
    "print(f\"Adjectives: {adjectives}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Named Entity Recognition (NER) - Production Quality\n",
    "\n",
    "SpaCy's NER is **far superior** to NLTK. Used by major tech companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Apple Inc. CEO Tim Cook announced in Cupertino that the company earned $365.8 billion in 2021.\n",
    "Microsoft and Google are also based in the United States. \n",
    "On January 15, 2024, the European Union imposed new regulations.\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"Named Entities Detected:\\n\")\n",
    "print(f\"{'Entity':<30} {'Label':<15} {'Description':<40}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:<30} {ent.label_:<15} {spacy.explain(ent.label_):<40}\")\n",
    "\n",
    "# Extract by entity type\n",
    "organizations = [ent.text for ent in doc.ents if ent.label_ == \"ORG\"]\n",
    "locations = [ent.text for ent in doc.ents if ent.label_ == \"GPE\"]\n",
    "dates = [ent.text for ent in doc.ents if ent.label_ == \"DATE\"]\n",
    "money = [ent.text for ent in doc.ents if ent.label_ == \"MONEY\"]\n",
    "people = [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "\n",
    "print(f\"\\nOrganizations: {organizations}\")\n",
    "print(f\"People:        {people}\")\n",
    "print(f\"Locations:     {locations}\")\n",
    "print(f\"Dates:         {dates}\")\n",
    "print(f\"Money:         {money}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Stopwords & Punctuation Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total stopwords in SpaCy: {len(STOP_WORDS)}\")\n",
    "print(f\"Sample stopwords: {list(STOP_WORDS)[:20]}\\n\")\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog! It's amazing!!!\"\n",
    "doc = nlp(text)\n",
    "\n",
    "print(f\"Original: {text}\\n\")\n",
    "\n",
    "# Remove stopwords\n",
    "without_stopwords = [token.text for token in doc if not token.is_stop]\n",
    "print(f\"Without stopwords: {' '.join(without_stopwords)}\")\n",
    "\n",
    "# Remove punctuation\n",
    "without_punct = [token.text for token in doc if not token.is_punct]\n",
    "print(f\"Without punct:     {' '.join(without_punct)}\")\n",
    "\n",
    "# Remove both\n",
    "cleaned = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "print(f\"Both removed:      {' '.join(cleaned)}\")\n",
    "\n",
    "# Only alphabetic\n",
    "alpha_only = [token.text for token in doc if token.is_alpha]\n",
    "print(f\"Only alphabetic:   {' '.join(alpha_only)}\")\n",
    "\n",
    "# Content words only\n",
    "content = [token.text for token in doc \n",
    "           if not token.is_stop and not token.is_punct and token.is_alpha]\n",
    "print(f\"Content words:     {' '.join(content)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Dependency Parsing - Understand Grammar\n",
    "\n",
    "This is where SpaCy **crushes** NLTK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The CEO of Apple announced new products yesterday\"\n",
    "doc = nlp(sentence)\n",
    "\n",
    "print(\"Dependency Parsing:\\n\")\n",
    "print(f\"{'Token':<12} {'Dependency':<12} {'Head':<12} {'Children':<30}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for token in doc:\n",
    "    children = [child.text for child in token.children]\n",
    "    print(f\"{token.text:<12} {token.dep_:<12} {token.head.text:<12} {', '.join(children):<30}\")\n",
    "\n",
    "# Extract Subject-Verb-Object\n",
    "print(\"\\nSubject-Verb-Object Extraction:\")\n",
    "for token in doc:\n",
    "    if token.dep_ == \"ROOT\":  # Main verb\n",
    "        subject = [child.text for child in token.children if child.dep_ == \"nsubj\"]\n",
    "        obj = [child.text for child in token.children if child.dep_ == \"dobj\"]\n",
    "        print(f\"Subject: {subject}\")\n",
    "        print(f\"Verb:    {token.text}\")\n",
    "        print(f\"Object:  {obj}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Production-Ready Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_spacy(text, \n",
    "                     lowercase=True,\n",
    "                     remove_stopwords=True,\n",
    "                     remove_punct=True,\n",
    "                     remove_emails=True,\n",
    "                     remove_urls=True,\n",
    "                     lemmatize=True,\n",
    "                     only_alpha=True,\n",
    "                     min_token_len=2):\n",
    "    \"\"\"\n",
    "    Production-grade text preprocessing with SpaCy\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str - Input text\n",
    "    lowercase : bool - Convert to lowercase\n",
    "    remove_stopwords : bool - Remove stopwords\n",
    "    remove_punct : bool - Remove punctuation\n",
    "    remove_emails : bool - Remove emails\n",
    "    remove_urls : bool - Remove URLs\n",
    "    lemmatize : bool - Lemmatize tokens\n",
    "    only_alpha : bool - Keep only alphabetic\n",
    "    min_token_len : int - Minimum token length\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list : Processed tokens\n",
    "    \"\"\"\n",
    "    # Process with SpaCy\n",
    "    doc = nlp(text.lower() if lowercase else text)\n",
    "    \n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        # Skip based on filters\n",
    "        if remove_stopwords and token.is_stop:\n",
    "            continue\n",
    "        if remove_punct and token.is_punct:\n",
    "            continue\n",
    "        if remove_urls and token.like_url:\n",
    "            continue\n",
    "        if remove_emails and token.like_email:\n",
    "            continue\n",
    "        if only_alpha and not token.is_alpha:\n",
    "            continue\n",
    "        if len(token.text) < min_token_len:\n",
    "            continue\n",
    "        \n",
    "        # Add lemma or original\n",
    "        tokens.append(token.lemma_ if lemmatize else token.text)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Test\n",
    "test_text = \"\"\"\n",
    "Natural Language Processing (NLP) is AMAZING!!! \n",
    "Visit https://spacy.io for more info. Contact: test@email.com\n",
    "The researchers are studying advanced AI techniques.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Original:\")\n",
    "print(test_text)\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "print(\"Full preprocessing:\")\n",
    "result = preprocess_spacy(test_text)\n",
    "print(result)\n",
    "print(f\"Token count: {len(result)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "print(\"Minimal (keep stopwords):\")\n",
    "result_min = preprocess_spacy(test_text, remove_stopwords=False)\n",
    "print(result_min)\n",
    "print(f\"Token count: {len(result_min)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Batch Processing - 10x Faster!\n",
    "\n",
    "For processing multiple documents, use `nlp.pipe()` - it's **10x faster** than loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents\n",
    "documents = [\n",
    "    \"Apple is releasing new products\",\n",
    "    \"Microsoft announced quarterly earnings\",\n",
    "    \"Google's AI research is advancing\",\n",
    "    \"Amazon dominates cloud computing\",\n",
    "    \"Tesla's stock price increased\"\n",
    "] * 100  # 500 documents\n",
    "\n",
    "print(f\"Processing {len(documents)} documents...\\n\")\n",
    "\n",
    "# BAD: One-by-one (slow)\n",
    "start = time.time()\n",
    "docs_slow = [nlp(text) for text in documents]\n",
    "time_slow = time.time() - start\n",
    "print(f\"‚ùå One-by-one:  {time_slow:.3f} seconds\")\n",
    "\n",
    "# GOOD: Batch with nlp.pipe()\n",
    "start = time.time()\n",
    "docs_fast = list(nlp.pipe(documents, batch_size=50))\n",
    "time_fast = time.time() - start\n",
    "print(f\"‚úÖ Batch (pipe): {time_fast:.3f} seconds\")\n",
    "print(f\"\\nüöÄ Speedup: {time_slow/time_fast:.1f}x faster!\")\n",
    "\n",
    "# Extract entities efficiently\n",
    "all_entities = []\n",
    "for doc in nlp.pipe(documents[:10], batch_size=5):\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    all_entities.extend(entities)\n",
    "\n",
    "print(f\"\\nEntities found: {len(all_entities)}\")\n",
    "print(f\"Sample: {all_entities[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Disable Unused Components - Optimize Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full pipeline (slower)\n",
    "nlp_full = spacy.load(\"en_core_web_sm\")\n",
    "print(f\"Full pipeline: {nlp_full.pipe_names}\")\n",
    "\n",
    "# Minimal pipeline (faster) - only tokenizer\n",
    "nlp_minimal = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "print(f\"Minimal: {nlp_minimal.pipe_names}\")\n",
    "\n",
    "# NER only\n",
    "nlp_ner = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\"])\n",
    "print(f\"NER only: {nlp_ner.pipe_names}\")\n",
    "\n",
    "# Benchmark\n",
    "text = \"Apple Inc. is a technology company\" * 100\n",
    "\n",
    "start = time.time()\n",
    "doc = nlp_full(text)\n",
    "full_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "doc = nlp_minimal(text)\n",
    "minimal_time = time.time() - start\n",
    "\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"Full pipeline:    {full_time:.4f}s\")\n",
    "print(f\"Minimal pipeline: {minimal_time:.4f}s\")\n",
    "print(f\"Speedup:          {full_time/minimal_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. SpaCy vs NLTK - Direct Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The researchers are studying NLP techniques in California\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"NLTK Way (Multiple steps, complex):\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Step 1: Tokenize\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "# Step 2: Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered = [w for w in tokens if w not in stop_words and w.isalpha()]\n",
    "\n",
    "# Step 3: Lemmatize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "result = [lemmatizer.lemmatize(w) for w in filtered]\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SpaCy Way (One line, simple):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# SpaCy: ONE line!\n",
    "doc = nlp(text.lower())\n",
    "spacy_result = [token.lemma_ for token in doc \n",
    "                if not token.is_stop and not token.is_punct and token.is_alpha]\n",
    "\n",
    "print(f\"\\nResult: {spacy_result}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SpaCy Bonus (NLTK doesn't have):\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Entities: {[(ent.text, ent.label_) for ent in doc.ents]}\")\n",
    "print(f\"POS tags: {[(token.text, token.pos_) for token in doc][:5]}\")\n",
    "print(f\"Dependencies: {[(token.text, token.dep_) for token in doc][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Real-World Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Case 1: Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_classification(text):\n",
    "    \"\"\"Preprocess for ML classification\"\"\"\n",
    "    doc = nlp(text.lower())\n",
    "    return [token.lemma_ for token in doc \n",
    "            if not token.is_stop \n",
    "            and not token.is_punct \n",
    "            and token.is_alpha\n",
    "            and len(token) > 2]\n",
    "\n",
    "reviews = [\n",
    "    \"This product is amazing! Best purchase ever.\",\n",
    "    \"Terrible quality. Very disappointed.\",\n",
    "    \"Average product, nothing special.\"\n",
    "]\n",
    "\n",
    "print(\"Text Classification Preprocessing:\\n\")\n",
    "for review in reviews:\n",
    "    processed = preprocess_for_classification(review)\n",
    "    print(f\"Original:  {review}\")\n",
    "    print(f\"Processed: {processed}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Case 2: Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_key_info(text):\n",
    "    \"\"\"Extract structured info from text\"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    return {\n",
    "        'organizations': [ent.text for ent in doc.ents if ent.label_ == \"ORG\"],\n",
    "        'people': [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"],\n",
    "        'locations': [ent.text for ent in doc.ents if ent.label_ == \"GPE\"],\n",
    "        'dates': [ent.text for ent in doc.ents if ent.label_ == \"DATE\"],\n",
    "        'money': [ent.text for ent in doc.ents if ent.label_ == \"MONEY\"]\n",
    "    }\n",
    "\n",
    "article = \"\"\"\n",
    "Tesla CEO Elon Musk announced on March 15, 2024 that the company \n",
    "will invest $10 billion in a new factory in Austin, Texas.\n",
    "\"\"\"\n",
    "\n",
    "info = extract_key_info(article)\n",
    "print(\"Extracted Information:\\n\")\n",
    "for key, values in info.items():\n",
    "    if values:\n",
    "        print(f\"{key.capitalize()}: {values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Case 3: Search/Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_search(query, document):\n",
    "    \"\"\"Preprocess query and document for search\"\"\"\n",
    "    def process(text):\n",
    "        doc = nlp(text.lower())\n",
    "        # Keep content words (nouns, verbs, adjectives)\n",
    "        return [token.lemma_ for token in doc \n",
    "                if token.pos_ in [\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\"]\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    return {\n",
    "        'query': process(query),\n",
    "        'document': process(document)\n",
    "    }\n",
    "\n",
    "query = \"best machine learning courses\"\n",
    "document = \"Learn machine learning with our comprehensive courses. The best way to master ML.\"\n",
    "\n",
    "result = preprocess_for_search(query, document)\n",
    "print(\"Search Preprocessing:\")\n",
    "print(f\"Query:    {result['query']}\")\n",
    "print(f\"Document: {result['document']}\")\n",
    "\n",
    "# Calculate overlap\n",
    "overlap = set(result['query']) & set(result['document'])\n",
    "print(f\"\\nMatching: {overlap}\")\n",
    "print(f\"Score: {len(overlap)/len(result['query']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Production Best Practices\n",
    "\n",
    "### ‚úÖ DO's\n",
    "\n",
    "1. **Use `nlp.pipe()` for batches** - 10x faster than loops\n",
    "2. **Disable unused components** - saves memory/time\n",
    "3. **Choose right model**:\n",
    "   - `sm`: Fast, most tasks\n",
    "   - `md`: Word vectors needed\n",
    "   - `lg`: Better accuracy\n",
    "   - `trf`: Best accuracy (slower)\n",
    "4. **Cache nlp object** - load once, reuse\n",
    "5. **Work with Doc/Token objects** - don't convert to strings\n",
    "\n",
    "### ‚ùå DON'Ts\n",
    "\n",
    "1. **Don't process one-by-one** - use `nlp.pipe()`\n",
    "2. **Don't load model in loops** - load once\n",
    "3. **Don't use string operations** - use token attributes\n",
    "4. **Don't convert to strings unnecessarily**\n",
    "5. **Don't load full pipeline if not needed**\n",
    "\n",
    "### Task-Specific Configurations\n",
    "\n",
    "| Task | Model | Components |\n",
    "|------|-------|------------|\n",
    "| Classification | sm/md | tok2vec, tagger, lemmatizer |\n",
    "| NER | lg/trf | tok2vec, ner |\n",
    "| Sentiment | md | tok2vec, tagger, lemmatizer |\n",
    "| Parsing | lg | all |\n",
    "| Tokenization | sm | tokenizer only |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Key Takeaways\n",
    "\n",
    "### Why SpaCy is Industry Standard\n",
    "\n",
    "1. **Speed**: 10-100x faster than NLTK\n",
    "2. **Accuracy**: State-of-the-art models\n",
    "3. **Simplicity**: One call does everything\n",
    "4. **Features**: NER, parsing, vectors built-in\n",
    "5. **Production**: Used by Google, Meta, Microsoft\n",
    "\n",
    "### Migration from NLTK\n",
    "\n",
    "```python\n",
    "# OLD (NLTK) - Multiple steps\n",
    "tokens = word_tokenize(text.lower())\n",
    "tokens = [w for w in tokens if w not in stopwords.words('english')]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "\n",
    "# NEW (SpaCy) - One line\n",
    "doc = nlp(text.lower())\n",
    "tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "```\n",
    "\n",
    "### Resources\n",
    "\n",
    "- Documentation: https://spacy.io\n",
    "- Free Course: https://course.spacy.io\n",
    "- Models: https://spacy.io/models\n",
    "- GitHub: https://github.com/explosion/spaCy\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Install SpaCy and download models\n",
    "2. Replace NLTK code with SpaCy\n",
    "3. Optimize with `nlp.pipe()` and component disabling\n",
    "4. Explore word vectors and similarity\n",
    "5. Deploy to production! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
