{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "333563f5",
   "metadata": {},
   "source": [
    "\n",
    "## 1️⃣ What is TF-IDF (quick recap)\n",
    "\n",
    "For a word **w** in document **d**:\n",
    "\n",
    "### **TF (Term Frequency)**\n",
    "$$\n",
    "[\n",
    "TF(w, d) = \\frac{\\text{count of w in d}}{\\text{total words in d}}\n",
    "]\n",
    "$$\n",
    "### **IDF (Inverse Document Frequency)**\n",
    "$$\n",
    "[\n",
    "IDF(w) = \\log\\left(\\frac{N}{df(w)}\\right)\n",
    "]\n",
    "$$\n",
    "Where:\n",
    "\n",
    "* ( N ) = total number of documents\n",
    "* ( df(w) ) = number of documents containing word ( w )\n",
    "\n",
    "### **TF-IDF**\n",
    "$$\n",
    "[\n",
    "TFIDF(w, d) = TF(w, d) \\times IDF(w)\n",
    "]\n",
    "$$\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f422318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "\n",
    "class TFIDFVectorizer:\n",
    "    \"\"\"\n",
    "    A simple TF-IDF Vectorizer implemented from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.vocabulary_: dict[str, int] = {}\n",
    "        self.idf_: dict[str, float] = {}\n",
    "        self.fitted: bool = False\n",
    "\n",
    "    def _tokenize(self, text: str) -> list[str]:\n",
    "        return text.lower().split()\n",
    "\n",
    "    def fit(self, documents: list[str]) -> None:\n",
    "        \"\"\"\n",
    "        Learn vocabulary and IDF values from the documents.\n",
    "        \"\"\"\n",
    "        tokenized_docs: list[list[str]] = [self._tokenize(doc) for doc in documents]\n",
    "        num_docs: int = len(tokenized_docs)\n",
    "\n",
    "        # ---------- OPTIMIZED DF COMPUTATION ----------\n",
    "        df: dict[str, int] = defaultdict(int)\n",
    "\n",
    "        for doc_tokens in tokenized_docs:\n",
    "            unique_words = set(doc_tokens)      # O(L)\n",
    "            for word in unique_words:           # O(U)\n",
    "                df[word] += 1\n",
    "\n",
    "        # Build vocabulary\n",
    "        self.vocabulary_ = {\n",
    "            word: idx for idx, word in enumerate(df.keys())\n",
    "        }\n",
    "\n",
    "        # Compute IDF\n",
    "        self.idf_ = {\n",
    "            word: math.log(num_docs / df[word])\n",
    "            for word in df\n",
    "        }\n",
    "\n",
    "        self.fitted = True\n",
    "\n",
    "    def _compute_tf(self, tokens: list[str]) -> dict[str, float]:\n",
    "        tf: dict[str, float] = {}\n",
    "        token_count = Counter(tokens)\n",
    "        total_tokens = len(tokens)\n",
    "\n",
    "        for word, count in token_count.items():\n",
    "            tf[word] = count / total_tokens\n",
    "\n",
    "        return tf\n",
    "\n",
    "    def transform(self, documents: list[str]) -> list[list[float]]:\n",
    "        \"\"\"\n",
    "        Transform documents to TF-IDF vectors.\n",
    "        - Find the tokens.\n",
    "        - Find the TF for them.\n",
    "        - generate the list length of whole vocabulary.\n",
    "        - for each word in the tf. => \n",
    "            - Find the id of the word from the vocabulary.\n",
    "            - fill the list with the tf*idf values.\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"The vectorizer must be fitted before calling transform().\")\n",
    "\n",
    "        tfidf_vectors: list[list[float]] = []\n",
    "\n",
    "        for doc in documents:\n",
    "            tokens = self._tokenize(doc)\n",
    "            tf = self._compute_tf(tokens)\n",
    "\n",
    "            vector = [0.0] * len(self.vocabulary_)\n",
    "\n",
    "            for word, tf_value in tf.items():\n",
    "                if word in self.vocabulary_:\n",
    "                    idx = self.vocabulary_[word]\n",
    "                    vector[idx] = tf_value * self.idf_[word]\n",
    "\n",
    "            tfidf_vectors.append(vector)\n",
    "\n",
    "        return tfidf_vectors\n",
    "\n",
    "    def fit_transform(self, documents: list[str]) -> list[list[float]]:\n",
    "        \"\"\"\n",
    "        Fit to data, then transform it.\n",
    "        \"\"\"\n",
    "        self.fit(documents)\n",
    "        return self.transform(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0abb454e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1013662770270411, 0.0, 0.1013662770270411, 0.1013662770270411, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.1013662770270411, 0.1013662770270411, 0.27465307216702745, 0.0, 0.0]\n",
      "[0.1013662770270411, 0.0, 0.0, 0.0, 0.0, 0.27465307216702745, 0.27465307216702745]\n"
     ]
    }
   ],
   "source": [
    "documents = [\n",
    "    \"I love machine learning\",\n",
    "    \"I love deep learning\",\n",
    "    \"Machine learning is fun\"\n",
    "]\n",
    "\n",
    "vectorizer = TFIDFVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "for vec in tfidf_matrix:\n",
    "    print(vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efb07ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9707fac0",
   "metadata": {},
   "source": [
    "N - number of documnets\n",
    "L - avg numbe rof words in the document\n",
    "V - vocabulary size\n",
    "\n",
    "\n",
    "\n",
    "| Step / Method          | Time Complexity    | Simple meaning                  |\n",
    "| ---------------------- | ------------------ | ------------------------------- |\n",
    "| Tokenization           | O(N × L)           | Read every word once            |\n",
    "| DF computation         | O(N × L)           | Count unique words per document |\n",
    "| Vocabulary + IDF       | O(V)               | One pass over unique words      |\n",
    "| TF computation         | O(N × L)           | Count words per document        |\n",
    "| TF-IDF vector creation | O(N × (V + L))     | Build vectors of size V         |\n",
    "| **fit() total**        | **O(N × L)**       | Training is linear              |\n",
    "| **transform() total**  | **O(N × (V + L))** | Vector creation dominates       |\n",
    "\n",
    "- space \n",
    "\n",
    "| Component       | Space Complexity | Simple meaning            |\n",
    "| --------------- | ---------------- | ------------------------- |\n",
    "| Tokenized docs  | O(N × L)         | Store words temporarily   |\n",
    "| Vocabulary      | O(V)             | One entry per unique word |\n",
    "| IDF values      | O(V)             | One value per word        |\n",
    "| TF-IDF matrix   | **O(N × V)**     | Dense document vectors    |\n",
    "| **Total space** | **O(N × V)**     | Memory-heavy part         |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
