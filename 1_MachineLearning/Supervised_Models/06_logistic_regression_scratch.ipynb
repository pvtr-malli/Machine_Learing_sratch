{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Logistic Regression From Scratch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from sklearn.datasets import make_blobs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# testing place: \n",
                "import numpy as np\n",
                "\n",
                "# NOTE: Main chnage from linear is -> wrap the hypothesis function with the sigmoid fnction \n",
                "# and predict func change thats it \n",
                "\n",
                "class LogisticRegression:\n",
                "    def __init__(self, learning_rate=0.1, iteration=1000):\n",
                "        self.X = None\n",
                "        self.y = None\n",
                "        self.m = None \n",
                "        self.n = None \n",
                "        self.weights = None \n",
                "        self.max_iteration = iteration\n",
                "        self.learning_rate = learning_rate\n",
                "\n",
                "    def hypothesis(self, X):\n",
                "        return np.dot(X, self.weights) # w = (m,1) x=(n,m)\n",
                "    \n",
                "    def sigmoid(self, z):\n",
                "        return 1 / (1 + np.exp(-z))\n",
                "\n",
                "    def cost_function(self, y_pred:np.ndarray):\n",
                "        # binary cross entropy -> - sum(y * log(y_pred) + (1-y) log(1- y_pred))\n",
                "        eps = 1e-9  # numerical stability\n",
                "        # WHY: y_pred = 0 or 1\n",
                "        # log(0) -> inf -> this cause Nan cost\n",
                "        return (-1/self.m) * np.sum((self.y * np.log(y_pred + eps)) + ((1-self.y * np.log(1 - y_pred + eps))))\n",
                "\n",
                "    def train(self, X: np.ndarray, y:np.ndarray):\n",
                "        # need to include bias\n",
                "        try:\n",
                "            y.shape[1]\n",
                "        except IndexError as e:\n",
                "            # we need to change it to the 1 D array, not a list.\n",
                "            print(\"ERROR: Target array should be a one dimentional array not a list\"\n",
                "                  \"----> here the target value not in the shape of (n,1). \\nShape ({shape_y_0},1) and {shape_y} not match\"\n",
                "                  .format(shape_y_0 = y.shape[0] , shape_y = y.shape))\n",
                "            return \n",
                "        # m is number of training samples.\n",
                "        self.m  = X.shape[0]\n",
                "        # n is number of features/columns/dependant variables.\n",
                "        self.n = X.shape[1]\n",
                "\n",
                "        # Set the initial weight.\n",
                "        self.w = np.zeros((self.n , 1))\n",
                "\n",
                "        for ite in range(self.max_iteration):\n",
                "            # find hte y_pred \n",
                "            # findhte cost\n",
                "            # update the weights \n",
                "            y_pred = self.sigmoid(self.hypothesis(self.X))\n",
                "\n",
                "            cost = self.cost_function(y_pred)\n",
                "\n",
                "            # update the weights:\n",
                "            # sum( y_pred - y ) . X\n",
                "            dw = (1/self.m) * np.dot(self.X.T, (y_pred - self.y))\n",
                "            self.weights = self.weights - (self.learning_rate * dw)\n",
                "\n",
                "            if ite % 100 == 0:\n",
                "                print(f\"iteration {ite} -> cost {cost}\")\n",
                "\n",
                "    def predict(self, X_test):\n",
                "        test_X = np.insert(X_test, 0, 1, axis=1)\n",
                "        y_pred = self.sigmoid(self.hypothesis(test_X))\n",
                "        y_pred_class = y_pred >=0.5\n",
                "        return y_pred_class\n",
                "        \n",
                "    \n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LogisticRegression:\n",
                "    \"\"\"\n",
                "    1. i need LogisticRegression class\n",
                "    2. I need __init__ function with learning rate and #iteration parameter\n",
                "    3. I need train function with X,y parameter\n",
                "    4. I need predict function\n",
                "    5. I need sigmoid function for gradiant decent and hypothesis\n",
                "    \"\"\"\n",
                "    def __init__(self, learning_rate = 0.1, iteration = 10000):\n",
                "        \"\"\"\n",
                "        :param learning_rate: A samll value needed for gradient decent, default value id 0.1.\n",
                "        :param iteration: Number of training iteration, default value is 10,000.\n",
                "        \"\"\"\n",
                "        self.lr = learning_rate\n",
                "        self.it = iteration\n",
                "    \n",
                "    def cost_function(self, y, y_pred):\n",
                "        \"\"\"\n",
                "        :param y: Original target value.\n",
                "        :param y_pred: predicted target value.\n",
                "        \"\"\"\n",
                "        return -1 / self.m * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
                "\n",
                "    # hypothesis function.   \n",
                "    def sigmoid(self, z):\n",
                "        \"\"\"\n",
                "        :param z: Value to calculate sigmoid.\n",
                "        \"\"\"\n",
                "        return 1 / (1 + np.exp(-z))\n",
                "\n",
                "    def train(self, X, y):\n",
                "        \"\"\"\n",
                "        :param X: training data feature values ---> N Dimentional vector.\n",
                "        :param y: training data target value -----> 1 Dimentional array.\n",
                "        \"\"\"\n",
                "        # Target value should be in the shape of (n, 1) not (n, ).\n",
                "        # So, this will check that and change the shape to (n, 1), if not.\n",
                "        try:\n",
                "            y.shape[1]\n",
                "        except IndexError as e:\n",
                "            # we need to change it to the 1 D array, not a list.\n",
                "            print(\"ERROR: Target array should be a one dimentional array not a list\"\n",
                "                  \"----> here the target value not in the shape of (n,1). \\nShape ({shape_y_0},1) and {shape_y} not match\"\n",
                "                  .format(shape_y_0 = y.shape[0] , shape_y = y.shape))\n",
                "            return \n",
                "        # m is number of training samples.\n",
                "        self.m  = X.shape[0]\n",
                "        # n is number of features/columns/dependant variables.\n",
                "        self.n = X.shape[1]\n",
                "\n",
                "        # Set the initial weight.\n",
                "        self.w = np.zeros((self.n , 1))\n",
                "        # bias.\n",
                "        self.b = 0\n",
                "\n",
                "        for it in range(1, self.it+1):\n",
                "            # 1. Find the predicted value.\n",
                "            # 2. Find the Cost function.\n",
                "            # 3. Find the derivation of weights and bias.\n",
                "            # 4. Apply Gradient Decent.\n",
                "\n",
                "            y_pred = self.sigmoid(np.dot(X, self.w) + self.b)\n",
                "\n",
                "            cost = self.cost_function(y, y_pred)\n",
                "\n",
                "            # Derivation of w and b.\n",
                "            dw = 1 / self.m * np.dot(X.T , (y_pred - y))\n",
                "            db = 1 / self.m * np.sum(y_pred - y)\n",
                "\n",
                "            # Chnage the parameter value/ apply Gradient decent.\n",
                "            self.w = self.w - (self.lr * dw)\n",
                "            self.b = self.b - (self.lr * db)\n",
                "\n",
                "            if it % 1000 == 0:\n",
                "                print(\"The Cost function for the iteration {}----->{} :)\".format(it, cost))\n",
                "    \n",
                "    def predict(self, test_X):\n",
                "        \"\"\"\n",
                "        :param: test_X: Values need to be predicted.\n",
                "        \"\"\"\n",
                "        y_pred = self.sigmoid(np.dot(test_X, self.w) + self.b)\n",
                "        # output of the sigmoid function is between [0 - 1], then need to convert it to class values either 0 or 1.\n",
                "        y_pred_class = y_pred >=0.5\n",
                "\n",
                "        return y_pred_class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "====================================================================================================\n",
                        "Number of training data samples-----> 5000\n",
                        "Number of training features --------> 2\n",
                        "Shape of the target value ----------> (5000, 1)\n"
                    ]
                }
            ],
            "source": [
                "# Define the traning data.\n",
                "X, y = make_blobs(n_samples=5000, centers=2)\n",
                "\n",
                "# Chnage the shape of the target to 1 dimentional array.\n",
                "y = y[:, np.newaxis]\n",
                "\n",
                "print(\"=\"*100)\n",
                "print(\"Number of training data samples-----> {}\".format(X.shape[0]))\n",
                "print(\"Number of training features --------> {}\".format(X.shape[1]))\n",
                "print(\"Shape of the target value ----------> {}\".format(y.shape))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "====================================================================================================\n",
                        "The Cost function for the iteration 1000----->0.03602371339407513 :)\n",
                        "The Cost function for the iteration 2000----->0.03083958133641108 :)\n",
                        "The Cost function for the iteration 3000----->0.02804445307164748 :)\n",
                        "The Cost function for the iteration 4000----->0.026035002983647044 :)\n",
                        "The Cost function for the iteration 5000----->0.02442819074384672 :)\n",
                        "The Cost function for the iteration 6000----->0.023077020411096397 :)\n",
                        "The Cost function for the iteration 7000----->0.021909144479974677 :)\n",
                        "The Cost function for the iteration 8000----->0.02088269812859516 :)\n",
                        "The Cost function for the iteration 9000----->0.019970511567438353 :)\n",
                        "The Cost function for the iteration 10000----->0.019153415288694367 :)\n",
                        "====================================================================================================\n",
                        "Accuracy of the prediction is 0.9942\n"
                    ]
                }
            ],
            "source": [
                "#define the parameters\n",
                "param = {\n",
                "    \"learning_rate\" : 0.1,\n",
                "    \"iteration\" : 10000\n",
                "}\n",
                "print(\"=\"*100)\n",
                "log_reg = LogisticRegression(**param)\n",
                "\n",
                "# Train the model.\n",
                "log_reg.train(X, y) \n",
                "\n",
                "# Predict the values.\n",
                "y_pred = log_reg.predict(X)\n",
                "\n",
                "#calculate accuracy.\n",
                "acc = np.sum(y==y_pred)/X.shape[0]\n",
                "print(\"=\"*100)\n",
                "print(\"Accuracy of the prediction is {}\".format(acc))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Logistic Regression Using sklearn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.linear_model import LogisticRegression as LogisticRegression_sklearn\n",
                "from sklearn.metrics import accuracy_score"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "====================================================================================================\n",
                        "Number of training data samples-----> 5000\n",
                        "Number of training features --------> 2\n"
                    ]
                }
            ],
            "source": [
                "# data is already defined, going to use the same data for comparision.\n",
                "print(\"=\"*100)\n",
                "print(\"Number of training data samples-----> {}\".format(X.shape[0]))\n",
                "print(\"Number of training features --------> {}\".format(X.shape[1]))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "====================================================================================================\n",
                        "Accuracy of the prediction is 0.9978\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/pavithra/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:985: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
                        "  y = column_or_1d(y, warn=True)\n"
                    ]
                }
            ],
            "source": [
                "log_reg_sklearn = LogisticRegression_sklearn()\n",
                "log_reg_sklearn.fit(X, y)\n",
                "\n",
                "# predict the value\n",
                "y_pred_sklearn = log_reg_sklearn.predict(X)\n",
                "acc = accuracy_score(y, y_pred_sklearn)\n",
                "print(\"=\"*100)\n",
                "print(\"Accuracy of the prediction is {}\".format(acc))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [],
            "source": [
                "# The main different between this scratch and sklearn model is speed.\n",
                "# Sklearn model train and predict more faster than sractch code.\n",
                "#  anyways this scracth code is maily for understanding the behind math and logics :) :)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Test "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- NOTE: Main change is wrap sigmoid around the hypothesis function thats it\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from sklearn.datasets import make_blobs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "# testing place: \n",
                "import numpy as np\n",
                "\n",
                "class LogisticRegression:\n",
                "    def __init__(self, learning_rate=0.1, iteration=1000):\n",
                "        self.X = None\n",
                "        self.y = None\n",
                "        self.m = None \n",
                "        self.n = None \n",
                "        self.weights = None \n",
                "        self.max_iteration = iteration\n",
                "        self.learning_rate = learning_rate\n",
                "\n",
                "    def hypothesis(self, X):\n",
                "        return np.dot(X, self.weights) # w = (m,1) x=(n,m)\n",
                "    \n",
                "    def sigmoid(self, z):\n",
                "        return 1 / (1 + np.exp(-z))\n",
                "\n",
                "    def cost_function(self, y_pred:np.ndarray):\n",
                "        # binary cross entropy -> - sum(y * log(y_pred) + (1-y) log(1- y_pred))\n",
                "        eps = 1e-9  # numerical stability\n",
                "        # WHY: y_pred = 0 or 1\n",
                "        # log(0) -> inf -> this cause Nan cost\n",
                "        return (-1/self.n) * np.sum((self.y * np.log(y_pred + eps)) + ((1-self.y * np.log(1 - y_pred + eps))))\n",
                "\n",
                "    def train(self, X: np.ndarray, y:np.ndarray):\n",
                "        # need to include bias\n",
                "        X = np.insert(X, obj=0, values=1, axis=1)\n",
                "        self.X = X\n",
                "        self.y = y\n",
                "        self.n, self.m = X.shape\n",
                "        self.weights = np.zeros((self.m, 1))\n",
                "\n",
                "        for ite in range(self.max_iteration):\n",
                "            # find hte y_pred \n",
                "            # findhte cost\n",
                "            # update the weights \n",
                "            y_pred = self.sigmoid(self.hypothesis(self.X))\n",
                "\n",
                "            cost = self.cost_function(y_pred)\n",
                "\n",
                "            # update the weights:\n",
                "            # sum( y_pred - y ) . X\n",
                "            dw = (1/self.n) * np.dot(self.X.T, (y_pred - self.y))\n",
                "            self.weights = self.weights - (self.learning_rate * dw)\n",
                "\n",
                "            if ite % 100 == 0:\n",
                "                print(f\"iteration {ite} -> cost {cost}\")\n",
                "\n",
                "    def predict(self, X_test):\n",
                "        test_X = np.insert(X_test, 0, 1, axis=1)\n",
                "        y_pred = self.sigmoid(self.hypothesis(test_X))\n",
                "        y_pred_class = y_pred >=0.5\n",
                "        return y_pred_class\n",
                "        \n",
                "    \n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "====================================================================================================\n",
                        "Number of training data samples-----> 5000\n",
                        "Number of training features --------> 2\n",
                        "Shape of the target value ----------> (5000, 1)\n",
                        "====================================================================================================\n",
                        "iteration 0 -> cost -1.0\n",
                        "iteration 100 -> cost -5.973952685001934\n",
                        "iteration 200 -> cost -6.745714782661034\n",
                        "iteration 300 -> cost -7.21641909861579\n",
                        "iteration 400 -> cost -7.558248198979805\n",
                        "iteration 500 -> cost -7.827547711899027\n",
                        "iteration 600 -> cost -8.050026125215487\n",
                        "iteration 700 -> cost -8.239621346157712\n",
                        "iteration 800 -> cost -8.404755250471325\n",
                        "iteration 900 -> cost -8.55091571645533\n",
                        "====================================================================================================\n",
                        "Accuracy of the prediction is 1.0\n"
                    ]
                }
            ],
            "source": [
                "# Define the traning data.\n",
                "X, y = make_blobs(n_samples=5000, centers=2)\n",
                "\n",
                "# Chnage the shape of the target to 1 dimentional array.\n",
                "y = y[:, np.newaxis]\n",
                "\n",
                "print(\"=\"*100)\n",
                "print(\"Number of training data samples-----> {}\".format(X.shape[0]))\n",
                "print(\"Number of training features --------> {}\".format(X.shape[1]))\n",
                "print(\"Shape of the target value ----------> {}\".format(y.shape))\n",
                "\n",
                "#define the parameters\n",
                "param = {\n",
                "    \"learning_rate\" : 0.1,\n",
                "    \"iteration\" : 1000\n",
                "}\n",
                "print(\"=\"*100)\n",
                "log_reg = LogisticRegression(**param)\n",
                "\n",
                "# Train the model.\n",
                "log_reg.train(X, y) \n",
                "\n",
                "# Predict the values.\n",
                "y_pred = log_reg.predict(X)\n",
                "\n",
                "#calculate accuracy.\n",
                "acc = np.sum(y==y_pred)/X.shape[0]\n",
                "print(\"=\"*100)\n",
                "print(\"Accuracy of the prediction is {}\".format(acc))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "3.11.11",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
